{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f4d471e83982d52",
   "metadata": {},
   "source": [
    "# SSL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c306047a0b0172d",
   "metadata": {},
   "source": [
    "- Masked ResNet18 Sequential Training\n",
    "- masking ratio = 0.75\n",
    "- batch size: labeled 32, unlabeled 128, valid 128\n",
    "- simple classifier / deeper decoder\n",
    "<br/><br/>\n",
    "- optimizer: supervised, downstream, pretext -> Adam\n",
    "- weight decay: supervised 0, downstream 0, pretext 1e-5\n",
    "- learning rate: supervised 1e-3, downstream 1e-3, pretext 1e-3\n",
    "<br/><br/>\n",
    "- pretext epochs: 25\n",
    "- downstream,supervised epochs: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb6fb677927dc63",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T07:01:55.272520Z",
     "start_time": "2025-08-27T07:01:55.263649Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Callable\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabaec606fcbb2f7",
   "metadata": {},
   "source": [
    "### Check GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84ddf6a545b9bb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T07:01:56.467994Z",
     "start_time": "2025-08-27T07:01:55.325064Z"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d671e4b1efd5a0c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T07:01:56.687690Z",
     "start_time": "2025-08-27T07:01:56.535022Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set CUDA Device Number 0~7\n",
    "DEVICE_NUM = 0\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(DEVICE_NUM)\n",
    "    device = torch.device(\"cuda\")\n",
    "print(\"INFO: Using device -\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bbed2fb5760dec",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977f4445e6e1f5b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T07:01:56.705621Z",
     "start_time": "2025-08-27T07:01:56.697822Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataType(Enum):\n",
    "    LABELED_TRAIN = 0\n",
    "    UNLABELED_TRAIN = 1\n",
    "    VALID = 2\n",
    "    TEST = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860c19d70d3664c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T07:01:56.810639Z",
     "start_time": "2025-08-27T07:01:56.798065Z"
    }
   },
   "outputs": [],
   "source": [
    "class CIFAR10Dataset(CIFAR10):\n",
    "    def __init__(self, data_type: DataType, transform: Optional[Callable] = None, validation_split: float = 0.1, labeled_split: float = 0.1):\n",
    "\n",
    "        super().__init__('./data', train=(data_type != DataType.TEST), transform=transform, download=True)\n",
    "\n",
    "        if data_type != DataType.TEST:\n",
    "            np.random.seed(42)\n",
    "            indices = np.random.permutation(len(self.data))\n",
    "\n",
    "            # validation index\n",
    "            val_size = int(len(self.data) * validation_split)\n",
    "            val_indices = indices[:val_size]\n",
    "\n",
    "            # labeled, unlabeled index\n",
    "            train_indices = indices[val_size:]\n",
    "            labeled_size = int(len(train_indices) * labeled_split)\n",
    "            labeled_indices = train_indices[:labeled_size]\n",
    "            unlabeled_indices = train_indices[labeled_size:]\n",
    "\n",
    "            if data_type == DataType.LABELED_TRAIN:\n",
    "                self.data = self.data[labeled_indices]\n",
    "                self.targets = [self.targets[i] for i in labeled_indices]\n",
    "\n",
    "            elif data_type == DataType.UNLABELED_TRAIN:\n",
    "                self.data = self.data[unlabeled_indices]\n",
    "                self.targets = [-1] * len(unlabeled_indices)\n",
    "                # self.targets = torch.full_like(torch.from_numpy(unlabeled_indices), -1)\n",
    "\n",
    "            else:\n",
    "                self.data = self.data[val_indices]\n",
    "                self.targets = [self.targets[i] for i in val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a18b08bd88c1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T07:01:56.858920Z",
     "start_time": "2025-08-27T07:01:56.851681Z"
    }
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = (32, 32)\n",
    "IMG_NORM = dict(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "resizer = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(**IMG_NORM)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1deedc1906535f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T07:02:20.996088Z",
     "start_time": "2025-08-27T07:01:56.948352Z"
    }
   },
   "outputs": [],
   "source": [
    "labeled_data = CIFAR10Dataset(DataType.LABELED_TRAIN, transform=resizer)\n",
    "unlabeled_data = CIFAR10Dataset(DataType.UNLABELED_TRAIN, transform=resizer)\n",
    "valid_data = CIFAR10Dataset(DataType.VALID, transform=resizer)\n",
    "test_data = CIFAR10Dataset(DataType.TEST, transform=resizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a77f1ece9b70e8",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11851991dc5cf03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T07:02:21.010024Z",
     "start_time": "2025-08-27T07:02:21.004397Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set Batch Size\n",
    "class BatchSize:\n",
    "    labeled: int = 32\n",
    "    unlabeled: int = 128\n",
    "    valid: int = 128\n",
    "\n",
    "batch_config = BatchSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70548b0f458f1504",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T07:02:21.119392Z",
     "start_time": "2025-08-27T07:02:21.113083Z"
    }
   },
   "outputs": [],
   "source": [
    "labeled_loader = DataLoader(labeled_data, batch_size=batch_config.labeled, shuffle=True)\n",
    "unlabeled_loader = DataLoader(unlabeled_data, batch_size=batch_config.unlabeled, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_config.valid, shuffle=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b8699a0f10a83a",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f870030a57e767f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T07:02:21.198031Z",
     "start_time": "2025-08-27T07:02:21.174093Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, supervised: bool = True, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.supervised = supervised\n",
    "        self.patch_size = 4\n",
    "        self.mask_ratio = 0.6\n",
    "        self.embed_dim = 512\n",
    "        self.in_channels = 3\n",
    "\n",
    "        backbone = models.resnet18()\n",
    "        backbone.conv1 = nn.Conv2d(self.in_channels, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        backbone.maxpool = nn.Identity()\n",
    "        self.backbone = nn.Sequential(*list(backbone.children())[:-2])\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.embed_dim, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 32, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 3, 3, 1, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.embed_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def random_mask(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        num_patches_h = H // self.patch_size\n",
    "        num_patches_w = W // self.patch_size\n",
    "        total_patches = num_patches_h * num_patches_w\n",
    "\n",
    "        num_masked = int(total_patches * self.mask_ratio)\n",
    "\n",
    "        mask = torch.ones(B, 1, H, W, device=x.device)\n",
    "\n",
    "        for b in range(B):\n",
    "            rand_indices = torch.randperm(total_patches, device=x.device)[:num_masked]\n",
    "\n",
    "            patch_h = rand_indices // num_patches_w\n",
    "            patch_w = rand_indices % num_patches_w\n",
    "\n",
    "            for h, w in zip(patch_h, patch_w):\n",
    "                h_start = h * self.patch_size\n",
    "                w_start = w * self.patch_size\n",
    "                mask[b, :, h_start:h_start+self.patch_size, w_start:w_start+self.patch_size] = 0\n",
    "\n",
    "        masked_x = x * mask\n",
    "\n",
    "        return masked_x, mask\n",
    "\n",
    "    def forward_encoder(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x\n",
    "\n",
    "    def forward_decoder(self, x):\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def forward_classifier(self, x):\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_encoder(x)\n",
    "        output = self.forward_classifier(x)\n",
    "        return output\n",
    "\n",
    "    def forward_pretext(self, x):\n",
    "        masked_x, mask = self.random_mask(x)\n",
    "        features = self.forward_encoder(masked_x)\n",
    "        reconstructed = self.forward_decoder(features)\n",
    "\n",
    "        return reconstructed, masked_x, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96e716b37c2856c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T07:02:21.682799Z",
     "start_time": "2025-08-27T07:02:21.262231Z"
    }
   },
   "outputs": [],
   "source": [
    "supervised_model = Model(supervised=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177e6486e18d8d22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T07:02:21.948818Z",
     "start_time": "2025-08-27T07:02:21.689642Z"
    }
   },
   "outputs": [],
   "source": [
    "self_supervised_model = Model(supervised=False).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d53531337dcd93",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad9d95121d13d74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T07:02:22.046270Z",
     "start_time": "2025-08-27T07:02:22.023218Z"
    }
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, labeled_loader, valid_loader, test_loader, unlabeled_loader=None, supervised=True):\n",
    "        self.supervised = supervised\n",
    "        self.labeled_loader = labeled_loader\n",
    "        self.unlabeled_loader = unlabeled_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.test_loader = test_loader\n",
    "\n",
    "        self.classification_loss = nn.CrossEntropyLoss()\n",
    "        self.reconstruction_loss = nn.MSELoss() if not supervised else None\n",
    "\n",
    "    def _train_step(self, model, batch, optimizer, device, pretext=False):\n",
    "        if pretext:\n",
    "            images, _ = batch\n",
    "            images = images.to(device)\n",
    "            reconstructed, _, _ = model.forward_pretext(images)\n",
    "            loss = self.reconstruction_loss(reconstructed, images)\n",
    "        else:\n",
    "            images, labels = batch\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = self.classification_loss(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def _train_epoch(self, model, optimizer, device, data_loader, epochs, pretext=False):\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "\n",
    "            for batch in data_loader:\n",
    "                loss = self._train_step(model, batch, optimizer, device, pretext)\n",
    "                total_loss += loss\n",
    "\n",
    "            avg_train_loss = total_loss / len(data_loader)\n",
    "            avg_valid_loss = self._evaluate(model, self.valid_loader, device, pretext)\n",
    "\n",
    "            print(f'Epoch {epoch+1}/{epochs}, 'f'Train Loss: {avg_train_loss:.4f}, 'f'Valid Loss: {avg_valid_loss:.4f}')\n",
    "\n",
    "    def _evaluate(self, model, data_loader, device, pretext=False, return_accuracy=False):\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in data_loader:\n",
    "                if pretext:\n",
    "                    images, _ = batch\n",
    "                    images = images.to(device)\n",
    "                    reconstructed, _, _ = model.forward_pretext(images)\n",
    "                    loss = self.reconstruction_loss(reconstructed, images)\n",
    "                else:\n",
    "                    images, labels = batch\n",
    "                    images = images.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    outputs = model(images)\n",
    "                    loss = self.classification_loss(outputs, labels)\n",
    "\n",
    "                    if return_accuracy:  # evaluate\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total += labels.size(0)\n",
    "                        correct += (predicted == labels).sum().item()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "        if return_accuracy:\n",
    "            accuracy = 100 * correct / total\n",
    "            return avg_loss, accuracy  # evaluate\n",
    "        return avg_loss  # validate\n",
    "\n",
    "    def train(self, model, device, optimizer, pre_optimizer=None, pretext_epochs=15, downstream_epochs=10):\n",
    "        model.to(device)\n",
    "        self.pre_optimizer = pre_optimizer\n",
    "\n",
    "        if self.supervised:\n",
    "            # Supervised\n",
    "            self._train_epoch(model, optimizer, device, self.labeled_loader, downstream_epochs, pretext=False)\n",
    "        else:\n",
    "            # Selfsupervised\n",
    "            print(\"Pretext Task Training\")\n",
    "            self._train_epoch(model, pre_optimizer, device, self.unlabeled_loader, pretext_epochs, pretext=True)\n",
    "\n",
    "            print(\"\\n\" + \"Downstream Task Training\")\n",
    "            self._train_epoch(model, optimizer, device, self.labeled_loader, downstream_epochs, pretext=False)\n",
    "\n",
    "    def evaluate(self, model, device):\n",
    "        model.to(device)\n",
    "\n",
    "        avg_loss, accuracy = self._evaluate(model, self.test_loader, device, pretext=False, return_accuracy=True)\n",
    "\n",
    "        print(f'Test Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ef378102228839",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T07:02:22.094365Z",
     "start_time": "2025-08-27T07:02:22.089817Z"
    }
   },
   "outputs": [],
   "source": [
    "supervised_trainer = Trainer(\n",
    "    labeled_loader,\n",
    "    valid_loader,\n",
    "    test_loader,\n",
    "    supervised=True\n",
    ")\n",
    "self_supervised_trainer = Trainer(\n",
    "    labeled_loader,\n",
    "    valid_loader,\n",
    "    test_loader,\n",
    "    unlabeled_loader,\n",
    "    supervised=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eeaaab0ea7d497",
   "metadata": {},
   "source": [
    "### Set Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14250c0b796622f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T07:02:22.189191Z",
     "start_time": "2025-08-27T07:02:22.181160Z"
    }
   },
   "outputs": [],
   "source": [
    "sl_optimizer = torch.optim.Adam(\n",
    "    supervised_model.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975783c1031b05d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T12:57:47.218319Z",
     "start_time": "2025-08-27T12:57:47.203306Z"
    }
   },
   "outputs": [],
   "source": [
    "ssl_optimizer_pre = torch.optim.Adam(\n",
    "    self_supervised_model.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-5\n",
    ")\n",
    "ssl_optimizer_down = torch.optim.Adam(\n",
    "    self_supervised_model.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84a5bab02533bbf",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2318741b9edb29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T07:03:34.036236Z",
     "start_time": "2025-08-27T07:02:22.222839Z"
    }
   },
   "outputs": [],
   "source": [
    "supervised_trainer.train(\n",
    "    model=supervised_model,\n",
    "    device=device,\n",
    "    optimizer=sl_optimizer,\n",
    "    downstream_epochs=10\n",
    ")\n",
    "torch.save(supervised_model.state_dict(), 'supervised_model_sequential.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea815478eafdfbc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T09:08:38.805936Z",
     "start_time": "2025-08-27T07:03:34.124387Z"
    }
   },
   "outputs": [],
   "source": [
    "self_supervised_trainer.train(\n",
    "    model=self_supervised_model,\n",
    "    device=device,\n",
    "    optimizer=ssl_optimizer_down,\n",
    "    pre_optimizer=ssl_optimizer_pre,\n",
    "    pretext_epochs=20,\n",
    "    downstream_epochs=10\n",
    ")\n",
    "torch.save(self_supervised_model.state_dict(), 'self_supervised_model_sequential.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb549e332f589fb",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64122afb976bfd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T09:09:24.770101Z",
     "start_time": "2025-08-27T09:08:38.834237Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Supervised Model\")\n",
    "supervised_trainer.evaluate(supervised_model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7192da05d2eb84f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T09:10:09.861049Z",
     "start_time": "2025-08-27T09:09:24.866606Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nSelf-supervised Model\")\n",
    "self_supervised_trainer.evaluate(self_supervised_model, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
