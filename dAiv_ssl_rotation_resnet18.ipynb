{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f4d471e83982d52",
   "metadata": {},
   "source": [
    "# SSL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb6fb677927dc63",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from typing import Optional, Callable, Union\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "from tqdm.auto import tqdm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dabaec606fcbb2f7",
   "metadata": {},
   "source": [
    "### Check GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "d84ddf6a545b9bb8",
   "metadata": {},
   "source": "!nvidia-smi",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d671e4b1efd5a0c3",
   "metadata": {},
   "source": [
    "DEVICE_NUM = 0\n",
    "\n",
    "device = torch.device(f\"cuda:{DEVICE_NUM}\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"INFO: Using device -\", device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "28bbed2fb5760dec",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "977f4445e6e1f5b5",
   "metadata": {},
   "source": [
    "class DataType(Enum):\n",
    "    LABELED_TRAIN = 0\n",
    "    UNLABELED_TRAIN = 1\n",
    "    UNLABELED_VALID = 2\n",
    "    VALID = 3\n",
    "    TEST = 4"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "860c19d70d3664c7",
   "metadata": {},
   "source": [
    "torchvision.datasets.utils.tqdm = tqdm\n",
    "\n",
    "\n",
    "class CIFAR10Dataset(CIFAR10):\n",
    "    UNLABELED_DATA_TYPE = {\n",
    "        DataType.UNLABELED_TRAIN,\n",
    "        DataType.UNLABELED_VALID\n",
    "    }\n",
    "    _indices_cache = {}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        data_type: DataType,\n",
    "        validation_split: float = 0.1,\n",
    "        labeled_split: float = 0.1,\n",
    "        transform: Optional[Callable] = None,\n",
    "    ):\n",
    "        super().__init__(root, train=(data_type != DataType.TEST), transform=None, download=True)\n",
    "        self.transform = transform\n",
    "        self.data_type = data_type\n",
    "        self.is_unlabeled = data_type in self.UNLABELED_DATA_TYPE\n",
    "\n",
    "        if data_type == DataType.TEST:\n",
    "            self.indices = np.arange(len(self.data))\n",
    "        else:\n",
    "            self.indices = self._get_indices(validation_split, labeled_split)\n",
    "\n",
    "    def _get_indices(self, validation_split: float, labeled_split: float):\n",
    "        cache_key = (len(self.data), validation_split, labeled_split)\n",
    "\n",
    "        if cache_key not in self._indices_cache:\n",
    "            rng = np.random.default_rng(seed=42)\n",
    "            indices = rng.permutation(len(self.data))\n",
    "\n",
    "            val_size = int(len(self.data) * validation_split)\n",
    "            val_indices = indices[:val_size]\n",
    "            train_indices = indices[val_size:]\n",
    "\n",
    "            labeled_size = int(len(train_indices) * labeled_split)\n",
    "            labeled_indices = train_indices[:labeled_size]\n",
    "            unlabeled_indices = train_indices[labeled_size:]\n",
    "\n",
    "            self._indices_cache[cache_key] = {\n",
    "                DataType.LABELED_TRAIN: labeled_indices,\n",
    "                DataType.UNLABELED_TRAIN: unlabeled_indices,\n",
    "                DataType.UNLABELED_VALID: val_indices,\n",
    "                DataType.VALID: val_indices,\n",
    "            }\n",
    "\n",
    "        return self._indices_cache[cache_key][self.data_type]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.is_unlabeled:\n",
    "            img_idx, rotation_k = divmod(index, 4)\n",
    "            actual_idx = self.indices[img_idx]\n",
    "            target = rotation_k\n",
    "        else:\n",
    "            actual_idx = self.indices[index]\n",
    "            target = self.targets[actual_idx]\n",
    "\n",
    "        img = Image.fromarray(self.data[actual_idx])\n",
    "\n",
    "        if self.is_unlabeled and rotation_k > 0:\n",
    "            img = img.rotate(90 * rotation_k)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices) * (4 if self.is_unlabeled else 1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c5a18b08bd88c1b",
   "metadata": {},
   "source": [
    "DATA_ROOT = './data'\n",
    "\n",
    "IMG_SIZE = (32, 32)\n",
    "IMG_NORM = dict(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "resizer = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(**IMG_NORM)\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b1deedc1906535f",
   "metadata": {},
   "source": [
    "labeled_data = CIFAR10Dataset(DATA_ROOT, DataType.LABELED_TRAIN, transform=resizer)\n",
    "unlabeled_data = CIFAR10Dataset(DATA_ROOT, DataType.UNLABELED_TRAIN, transform=resizer)\n",
    "valid_data = CIFAR10Dataset(DATA_ROOT, DataType.VALID, transform=resizer)\n",
    "unlabeled_valid_data = CIFAR10Dataset(DATA_ROOT, DataType.UNLABELED_VALID, transform=resizer)\n",
    "test_data = CIFAR10Dataset(DATA_ROOT, DataType.TEST, transform=resizer)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "10a77f1ece9b70e8",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "id": "b11851991dc5cf03",
   "metadata": {},
   "source": [
    "# Set Batch Size\n",
    "class BatchSize:\n",
    "    labeled: int = 256\n",
    "    unlabeled: int = 1024\n",
    "    valid: int = 1024\n",
    "\n",
    "batch_config = BatchSize()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "70548b0f458f1504",
   "metadata": {},
   "source": [
    "labeled_loader = DataLoader(labeled_data, batch_size=batch_config.labeled, shuffle=True)\n",
    "unlabeled_loader = DataLoader(unlabeled_data, batch_size=batch_config.unlabeled, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_config.valid, shuffle=True)\n",
    "unlabeled_valid_loader = DataLoader(unlabeled_valid_data, batch_size=batch_config.valid, shuffle=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e4b8699a0f10a83a",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "6f870030a57e767f",
   "metadata": {},
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_classes: int = 10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = 512\n",
    "        self.in_channels = 3\n",
    "        self.is_pretext = False\n",
    "\n",
    "        backbone = models.resnet18()\n",
    "        backbone.conv1 = nn.Conv2d(self.in_channels, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        backbone.maxpool = nn.Identity()\n",
    "        self.backbone = nn.Sequential(*list(backbone.children())[:-2])\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.embed_dim, num_classes)\n",
    "        )\n",
    "\n",
    "        self.rotation_classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.embed_dim, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.backbone(x)\n",
    "        if self.is_pretext:\n",
    "            out = self.rotation_classifier(out)\n",
    "        else:\n",
    "            out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    @classmethod\n",
    "    def from_checkpoint(cls, ckpt_path: Union[str, Path], num_classes: int = 10):\n",
    "        model = cls(num_classes=num_classes)\n",
    "        state_dict = torch.load(ckpt_path, map_location='cpu')\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        print(f\"INFO: Backbone weights successfully loaded from: {ckpt_path}\")\n",
    "        return model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d2d53531337dcd93",
   "metadata": {},
   "source": "## Trainer Class"
  },
  {
   "cell_type": "code",
   "id": "aad9d95121d13d74",
   "metadata": {},
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model: nn.Module):\n",
    "        self.model = model\n",
    "        self.device = None\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def _train_one_epoch(\n",
    "        self,\n",
    "        data_loader: DataLoader,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        progress_bar: tqdm\n",
    "    ):\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for _, (images, labels) in enumerate(data_loader):\n",
    "            images, labels = images.to(self.device), labels.to(self.device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            current_loss = loss.item()\n",
    "            total_loss += current_loss\n",
    "            \n",
    "            progress_bar.set_postfix(loss=f\"{current_loss:.6f}\")\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        return total_loss / len(data_loader)\n",
    "\n",
    "    def _evaluate(self, data_loader: DataLoader, progress_bar: Optional[tqdm] = None):\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in data_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                if progress_bar:\n",
    "                    progress_bar.update(1)\n",
    "\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        accuracy = correct / total\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        train_loader: DataLoader,\n",
    "        valid_loader: DataLoader,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        epochs: int,\n",
    "        is_pretext: bool = False,\n",
    "        patience: int = 10,\n",
    "        save_top_k: int = 3,\n",
    "        save_path: Union[str, Path] = Path(\"./checkpoints\"),\n",
    "    ):\n",
    "        task_name = \"pretext\" if is_pretext else \"downstream\"\n",
    "        tqdm.write(f\"--- Starting Task: {task_name.capitalize()} ---\")\n",
    "\n",
    "        if hasattr(self.model, 'is_pretext'):\n",
    "            self.model.is_pretext = is_pretext\n",
    "\n",
    "        save_path = Path(save_path)\n",
    "        save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        top_k_checkpoints = []\n",
    "\n",
    "        train_length, valid_length = len(train_loader), len(valid_loader)\n",
    "        epochs_progress = tqdm(range(epochs), desc=\"Epochs\", position=0, leave=True)\n",
    "\n",
    "        with tqdm(total=train_length, desc=\"Training\", position=1, leave=False) as train_progress, \\\n",
    "             tqdm(total=valid_length, desc=\"Validation\", position=2, leave=False) as valid_progress:\n",
    "\n",
    "            for epoch in epochs_progress:\n",
    "                train_progress.reset()\n",
    "                valid_progress.reset()\n",
    "\n",
    "                # Training & Validation\n",
    "                train_loss = self._train_one_epoch(train_loader, optimizer, train_progress)\n",
    "                valid_loss, valid_acc = self._evaluate(valid_loader, valid_progress)\n",
    "\n",
    "                final_log = (\n",
    "                    f\"Epoch [{epoch+1:>{len(str(epochs))}}/{epochs}] | \"\n",
    "                    f\"Train Loss: {train_loss:.6f} | \"\n",
    "                    f\"Valid Loss: {valid_loss:.6f} | \"\n",
    "                    f\"Valid Acc: {valid_acc:.4%}\"\n",
    "                )\n",
    "                tqdm.write(final_log)\n",
    "\n",
    "                # Checkpoint saving logic\n",
    "                if valid_loss < best_val_loss:\n",
    "                    best_val_loss = valid_loss\n",
    "                    patience_counter = 0\n",
    "                    \n",
    "                    ckpt_path = save_path / f\"{task_name}_epoch_{epoch+1}_loss_{valid_loss:.6f}.pt\"\n",
    "                    torch.save(self.model.state_dict(), ckpt_path)\n",
    "                    top_k_checkpoints.append((valid_loss, ckpt_path))\n",
    "                    top_k_checkpoints.sort(key=lambda x: x[0])\n",
    "\n",
    "                    if len(top_k_checkpoints) > save_top_k:\n",
    "                        worst_checkpoint_path = top_k_checkpoints.pop()[1]\n",
    "                        if worst_checkpoint_path.exists():\n",
    "                            worst_checkpoint_path.unlink()\n",
    "                \n",
    "                else: # Early stopping logic\n",
    "                    patience_counter += 1\n",
    "\n",
    "                if patience_counter >= patience:\n",
    "                    tqdm.write(f\"\\nEarly stopping at epoch {epoch+1} as validation loss did not improve for {patience} epochs.\")\n",
    "                    break\n",
    "\n",
    "    def test(self, test_loader: DataLoader):\n",
    "        if hasattr(self.model, 'is_pretext'):\n",
    "            self.model.is_pretext = False\n",
    "\n",
    "        test_progress = tqdm(test_loader, desc=\"Testing\", leave=True)\n",
    "        test_loss, test_acc = self._evaluate(test_loader, progress_bar=test_progress)\n",
    "        test_progress.close()\n",
    "        print(f\"\\nTest Results | Test Loss: {test_loss:.6f} | Test Acc: {test_acc:.6%}\")\n",
    "\n",
    "    def to(self, device: torch.device):\n",
    "        self.model.to(device)\n",
    "        self.device = device\n",
    "        return self"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e84a5bab02533bbf",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d915ab8c66ef096b",
   "metadata": {},
   "source": "### Train Supervised Model"
  },
  {
   "cell_type": "code",
   "id": "e96e716b37c2856c",
   "metadata": {},
   "source": [
    "supervised_model = Model()\n",
    "\n",
    "# supervised_model = Model.from_checkpoint()\n",
    "\n",
    "supervised_trainer = Trainer(supervised_model).to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fc185410c8cc25fa",
   "metadata": {},
   "source": [
    "SL_DOWNSTREAM_LR = 1e-3\n",
    "\n",
    "sl_optimizer = AdamW(\n",
    "    supervised_model.parameters(),\n",
    "    lr=SL_DOWNSTREAM_LR,\n",
    "    weight_decay=1e-4\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2d2318741b9edb29",
   "metadata": {},
   "source": [
    "supervised_trainer.train(\n",
    "    train_loader=labeled_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    optimizer=sl_optimizer,\n",
    "    epochs=50,\n",
    "    is_pretext=False,\n",
    "    patience=5,\n",
    "    save_top_k=3,\n",
    "    save_path=\"./checkpoints/supervised\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2bfbb4f16d5b814a",
   "metadata": {},
   "source": "### Train Self-Supervised Model"
  },
  {
   "cell_type": "markdown",
   "id": "a4beedcf",
   "metadata": {},
   "source": [
    "#### A. Pretext"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Download pre-trained weights from Hugging Face Hub\n",
    "REPO_ID = \"haesol1013/2025-2_dAiv-SSL-Lecture\"\n",
    "FILE_NAME = \"pretext_pretrained.pt\"\n",
    "pretrained_weights_path = hf_hub_download(repo_id=REPO_ID, filename=FILE_NAME)"
   ],
   "id": "a0dbbe0e83fa9d60",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d83cb4ba0aacbe6b",
   "metadata": {},
   "source": [
    "# self_supervised_model = Model()\n",
    "\n",
    "self_supervised_model = Model().from_checkpoint(pretrained_weights_path)\n",
    "\n",
    "self_supervised_trainer = Trainer(self_supervised_model).to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3e6ee50e",
   "metadata": {},
   "source": [
    "SSL_PRETEXT_LR = 1e-3\n",
    "\n",
    "ssl_pretext_optim = AdamW(\n",
    "    self_supervised_model.parameters(),\n",
    "    lr=SSL_PRETEXT_LR,\n",
    "    weight_decay=1e-4\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ea815478eafdfbc4",
   "metadata": {},
   "source": [
    "PRETEXT_CKPT_PATH = Path(\"./checkpoints/self_supervised/pretext\")\n",
    "\n",
    "self_supervised_trainer.train(\n",
    "    train_loader=unlabeled_loader,\n",
    "    valid_loader=unlabeled_valid_loader,\n",
    "    optimizer=ssl_pretext_optim,\n",
    "    epochs=30,\n",
    "    is_pretext=True,\n",
    "    patience=5,\n",
    "    save_top_k=3,\n",
    "    save_path=PRETEXT_CKPT_PATH\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8d463926",
   "metadata": {},
   "source": [
    "#### B. Downstream"
   ]
  },
  {
   "cell_type": "code",
   "id": "ba5e5213",
   "metadata": {},
   "source": [
    "# Load the best checkpoint based on validation loss\n",
    "checkpoints = []\n",
    "for ckpt_file in PRETEXT_CKPT_PATH.glob(\"*.pt\"):\n",
    "    loss_str = ckpt_file.stem.split(\"_loss_\")[-1]\n",
    "    try:\n",
    "        loss_val = float(loss_str)\n",
    "        checkpoints.append((loss_val, ckpt_file))\n",
    "    except ValueError:\n",
    "        continue"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "642e22a7",
   "metadata": {},
   "source": [
    "# If no checkpoints found, initialize a new model\n",
    "if checkpoints:\n",
    "    best_loss, best_ckpt = min(checkpoints, key=lambda x: x[0])\n",
    "    self_supervised_model = Model.from_checkpoint(best_ckpt)\n",
    "else:\n",
    "    self_supervised_model = Model()\n",
    "    print(\"INFO: No pretext checkpoints found. Training from scratch.\")\n",
    "\n",
    "self_supervised_trainer = Trainer(self_supervised_model).to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "613b7a75",
   "metadata": {},
   "source": [
    "SSL_DOWNSTREAM_LR = 1e-4\n",
    "\n",
    "ssl_downstream_optimizer = AdamW(\n",
    "    self_supervised_model.parameters(),\n",
    "    lr=SSL_DOWNSTREAM_LR,\n",
    "    weight_decay=1e-4\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e52a7616002bda24",
   "metadata": {},
   "source": [
    "self_supervised_trainer.train(\n",
    "    train_loader=labeled_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    optimizer=ssl_downstream_optimizer,\n",
    "    epochs=30,\n",
    "    is_pretext=False,\n",
    "    patience=7,\n",
    "    save_top_k=3,\n",
    "    save_path=\"./checkpoints/self_supervised/downstream\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1eb549e332f589fb",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "id": "64122afb976bfd7",
   "metadata": {},
   "source": [
    "print(\"-- Supervised Model ---\")\n",
    "supervised_trainer.test(test_loader)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b7192da05d2eb84f",
   "metadata": {},
   "source": [
    "print(\"--- Self-supervised Model ---\")\n",
    "self_supervised_trainer.test(test_loader)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "54dc5318868a1ad9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
